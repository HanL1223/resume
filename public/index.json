[{"content":"Github link ðŸ”— Function URL SeniorAid55! ðŸ–¥ Frontend ðŸ’¾Backend\nCredit Members of team TP8 for subject FIT5120/22\nSkill invlove Python Pytorch Deep Learning Docker AWS Azure\nDescription As the final project of my Master Degree, this website is to tackle the topic of digital poverty by applying various functions. I mainly take part in the avoidng scam function where I adapt the transfer learning method, using a fine-tuned BERT model to predict the likelihood of a text message received by a senior is a scam or not\nTo add more content to the result, I also displayed the words that has the highest attention score to help our target audiences understand how the result is driven from, and a D3.js based visulization tool to support the understanding.\nKey Takeaways Combining DS skill to a real project. Experience on merging model develpment with backend(Django) and frontend(React). Collaborative Development. Experience a full-stack project development with a team,and using Github for version control in a team setting Agile Management. Here I also exposed to the full agile software development cycle of a product Cloud Computing. In-depth knowledge and hands on experience on both AWS(EC2,Route52,S3) and Azure (Machine Learning Studio,Virtual machines) New Domain knowledge. First time expose to developing a product in a more public domain setting ","permalink":"https://hanl1223.github.io/resume/projects/seniordigi/","summary":"Github link ðŸ”— Function URL SeniorAid55! ðŸ–¥ Frontend ðŸ’¾Backend\nCredit Members of team TP8 for subject FIT5120/22\nSkill invlove Python Pytorch Deep Learning Docker AWS Azure\nDescription As the final project of my Master Degree, this website is to tackle the topic of digital poverty by applying various functions. I mainly take part in the avoidng scam function where I adapt the transfer learning method, using a fine-tuned BERT model to predict the likelihood of a text message received by a senior is a scam or not","title":"Bridging the Digital Divide"},{"content":"Github link ðŸ”— Function URL ðŸ–¥ Code\nCredit Members of team TP8 for subject FIT5120/22\nSkill invlove Python Pandas Machine Learning\nDescription This is an additional function be added to the final project, a password strength evaluator, based on the dataset A and dataset B\nKey Takeaways Combining DS skill to a real project. Experience on merging model develpment with backend(Django) and frontend(React). Agile Management. Here I also exposed to the full agile software development cycle of a product Method to improve the models Where to improve Use full training set to increase accuracy and F1 perfromance, reducee False positive and Falues Negative Different method to preprocess the data Function Code # For Data manipulation import pandas as pd import numpy as np # For Data Visulization import matplotlib.pyplot as plt import seaborn as sns #For Modelling and evaluation from sklearn.model_selection import train_test_split,StratifiedKFold, cross_val_score from xgboost import XGBClassifier import lightgbm as lgb from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,BaggingClassifier,StackingClassifier from sklearn import metrics from sklearn.preprocessing import StandardScaler from sklearn.model_selection import GridSearchCV, RandomizedSearchCV from sklearn.neighbors import KNeighborsClassifier as KNN #For text preprocessing from sklearn.feature_extraction.text import TfidfVectorizer #For model parameter saving and loading import pickle # Import dataset df = pd.read_csv(\u0026#39;data.csv\u0026#39;,on_bad_lines=\u0026#39;skip\u0026#39;) #ignore badline from dataset df.isna().sum() df.duplicated().sum() #only 1 missing value, and no duplicate found,will drop the bad record directly df.dropna(inplace = True) #Import text data for all the weak passwords from rock you leak df2 = pd.read_csv(\u0026#39;rockyou.txt\u0026#39;,delimiter=\u0026#39;\\t\u0026#39;,header = None, names = [\u0026#39;password\u0026#39;],encoding=\u0026#39;ISO-8859-1\u0026#39;) df2.dropna(inplace = True) df2.drop_duplicates(inplace = True) df2[\u0026#39;strength\u0026#39;] = 0 df_full = pd.concat([df,df2],ignore_index=True) # Compute the value counts of the Gender column value_counts = df_full[\u0026#39;strength\u0026#39;].value_counts() # Set the number of samples to be drawn from each group n_samples = value_counts.min() # Group the dataframe by Gender and sample n_samples from each group sampled_df = df.groupby(\u0026#39;strength\u0026#39;).apply(lambda x: x.sample(n=n_samples)).reset_index(drop=True) # Print the sampled dataframe print(sampled_df) X = sampled_df[\u0026#39;password\u0026#39;] y = sampled_df[\u0026#39;strength\u0026#39;] #tokenize password vectorizer = TfidfVectorizer(analyzer = \u0026#39;char\u0026#39;) X = vectorizer.fit_transform(X) #Save the vectorizer for backend use with open(\u0026#34;vectorizer2.pkl\u0026#34;, \u0026#34;wb\u0026#34;) as f: pickle.dump(vectorizer, f) #Data preprocessing def train_val_test_split(X,y,ratio): X_train,X_,y_train,y_ = train_test_split(X,y,test_size=ratio,stratify=y,random_state=1) X_val,X_test,y_val,y_test = train_test_split(X_,y_,test_size=.5,stratify=y_,random_state=1) return X_train,X_val,X_test,y_train,y_val,y_test X_train,X_val,X_test,y_train,y_val,y_test = train_val_test_split(X,y,ratio=.25) #Model Selection with CV models = [] # Empty list to store all the models # Appending models into the list models.append((\u0026#34;Random forest\u0026#34;, RandomForestClassifier(random_state=1))) models.append((\u0026#34;Bagging\u0026#34;, BaggingClassifier(random_state=1))) models.append((\u0026#34;Xgboost\u0026#34;, XGBClassifier(random_state=1, eval_metric=\u0026#34;logloss\u0026#34;))) models.append((\u0026#34;lgbm\u0026#34;, lgb.LGBMClassifier(random_state=1))) models.append((\u0026#39;KNN\u0026#39;,KNN())) results = [] # Empty list to store all model\u0026#39;s CV scores names = [] # Empty list to store name of the models score = [] # loop through all models to get the mean cross validated score print(\u0026#34;\\n\u0026#34; \u0026#34;Cross-Validation Performance:\u0026#34; \u0026#34;\\n\u0026#34;) # Use F1 since for password evaluation I want to ensure to minimal both FP and FN rate for name, model in models: kfold = StratifiedKFold( n_splits=5, shuffle=True, random_state=1 ) # Setting number of splits equal to 5 cv_result = cross_val_score( estimator=model, X=X_train, y=y_train, scoring=\u0026#39;f1_macro\u0026#39;, cv=kfold ) results.append(cv_result) names.append(name) print(\u0026#34;{}: {}\u0026#34;.format(name, cv_result.mean())) #Form CV result, XGBoost generate around 98.4% marco f1 score #Using Xgboost provided the best f1_marco result, thus we can fine tune it # defining model - XGBoost Hyperparameter Tuning model = XGBClassifier(random_state=1, eval_metric=\u0026#34;logloss\u0026#34;) # Parameter grid to pass in RandomizedSearchCV param_grid = { \u0026#34;n_estimators\u0026#34;: np.arange(150, 300, 50), \u0026#34;learning_rate\u0026#34;: [0.0001,0.001,0.01,0.0015], \u0026#34;gamma\u0026#34;: [0, 3, 5,7], \u0026#34;subsample\u0026#34;: [0.5, 0.9,0.2,0.35], \u0026#39;reg_alpha\u0026#39;:[0,1], \u0026#39;reg_lambda\u0026#39;:[0,1] } # Calling RandomizedSearchCV randomized_cv = RandomizedSearchCV( estimator=model, param_distributions=param_grid, n_iter=20, # sample 20 setting scoring=\u0026#39;f1_macro\u0026#39;, cv=3, random_state=1, n_jobs=-1, ) # Fitting parameters in RandomizedSearchCV randomized_cv.fit(X_train, y_train) print( \u0026#34;Best parameters are {} with CV score={}:\u0026#34;.format( randomized_cv.best_params_, randomized_cv.best_score_ ) ) best_model = XGBClassifier( **randomized_cv.best_params_,eval_metric = \u0026#39;logloss\u0026#39;,random_state = 1 )# building model with best parameters print(\u0026#34;\\n\u0026#34; \u0026#34;Training Performance:\u0026#34; \u0026#34;\\n\u0026#34;) best_model.fit(X_train, y_train) scores = metrics.f1_score(y_train, best_model.predict(X_train),average=\u0026#39;macro\u0026#39;) print(\u0026#34; {}\u0026#34;.format( scores)) print(\u0026#34;\\n\u0026#34; \u0026#34;Validation Performance:\u0026#34; \u0026#34;\\n\u0026#34;) val_scores = metrics.f1_score(y_val, best_model.predict(X_val),average=\u0026#39;macro\u0026#39;) print(\u0026#34;{}\u0026#34;.format(val_scores)) # We can go ahead test the model if both training and validation performance are as expected y_pred = best_model.predict(X_test) metrics.f1_score(y_test,y_pred,average=\u0026#39;macro\u0026#39;) import pickle # Save the model into the pickle file for backend with open(\u0026#34;xgb_model.pkl\u0026#34;, \u0026#34;rb\u0026#34;) as f: pickle.dump(best_model,f) ","permalink":"https://hanl1223.github.io/resume/projects/security_check/","summary":"Github link ðŸ”— Function URL ðŸ–¥ Code\nCredit Members of team TP8 for subject FIT5120/22\nSkill invlove Python Pandas Machine Learning\nDescription This is an additional function be added to the final project, a password strength evaluator, based on the dataset A and dataset B\nKey Takeaways Combining DS skill to a real project. Experience on merging model develpment with backend(Django) and frontend(React). Agile Management. Here I also exposed to the full agile software development cycle of a product Method to improve the models Where to improve Use full training set to increase accuracy and F1 perfromance, reducee False positive and Falues Negative Different method to preprocess the data Function Code # For Data manipulation import pandas as pd import numpy as np # For Data Visulization import matplotlib.","title":"Password Strength Validation"},{"content":"Github link ðŸ”— Code\nCredit The notebook is inspire by Paper Skill invlove Python Collaborative Filtering Data preprocessing ScikitLearn\nProject Description This project aims to build a recommender system for book ratings. The system utilizes collaboration filtering methods and ensemble techniques to provide accurate recommendations to users based on their preferences. The project includes data exploration, preprocessing, model selection, model construction, and evaluation phases.\nThe recommender system employs three standalone models, implementing collaboration filtering methods. The models are fine-tuned and compared using cross-validation to select the best-performing model. The chosen model is then further optimized using random search cross-validation. The top-performing models, including SVD, SVDpp, and KNNwithZscore, are used to construct an ensemble model for testing.\nProject Takeaways Throughout the development of the Recommender System for Book Ratings, I gained several key takeaways:\nEnhanced Python skills, especially in data manipulation, model training, and evaluation. Deepened understanding of collaborative filtering techniques and their application in recommender systems. Explored data exploration techniques to identify trends and patterns in book ratings. Learned data preprocessing methods to improve data quality and reduce training time. Experienced model selection processes using cross-validation to identify the best-performing model. Practiced fine-tuning models with random search cross-validation for improved accuracy and robustness. Constructed an ensemble model by combining the top-performing models, showcasing the benefits of leveraging multiple models. Utilized evaluation metrics (MAE and RMSE) to quantitatively measure the performance of the recommender system. Identified limitations of collaborative filtering, such as cold-start issues and the presence of unseen books. Recognized the potential for future enhancements, including incorporating content-based methods and hybrid approaches. Considered scalability using frameworks like Spark to handle larger datasets and improve system performance.(Future improvement) Usage The recommender system can be used to provide book recommendations based on user preferences. Users can input their user ID or book name to receive personalized recommendations. The system utilizes collaborative filtering techniques and an ensemble model to generate accurate recommendations.\nTo obtain recommendations for a specific user:\nimport recommender_system user_id = \u0026#34;12345\u0026#34; # User ID for whom recommendations are required recommendations(df = test_data,user_id = int(user_id)) To obtain a potenical reader of a book:\nimport recommender_system book_name = \u0026#34;Angels \u0026amp; Demons (Robert Langdon, #1)\u0026#34; # Name of the book for which recommendations are required recommendations(df = test_data,book_name = book_name) id user_id\trating 33038\t1783\t4 26076\t1483\t4 193\t6\t3 26074\t1541\t3 930\t65\t3 For sample output ,we look at the reading history of top user 1783\ntrain[train[\u0026#39;user_id\u0026#39;] == 1483] user_id item_id rating book_name 135608 1483 399 4 The Da Vinci Code (Robert Langdon, #2) 135673 1483 456 4 Memoirs of a Geisha 135757 1483 1074 5 1984 135885 1483 1113 4 Harry Potter and the Order of the Phoenix (Har\u0026hellip; 136250 1483 1302 5 The Devil in the White City Since this user readed the Da Vinci Code, they might also interesting in its prequel. So the recommendation logically make sence\n","permalink":"https://hanl1223.github.io/resume/projects/recommender/","summary":"Github link ðŸ”— Code\nCredit The notebook is inspire by Paper Skill invlove Python Collaborative Filtering Data preprocessing ScikitLearn\nProject Description This project aims to build a recommender system for book ratings. The system utilizes collaboration filtering methods and ensemble techniques to provide accurate recommendations to users based on their preferences. The project includes data exploration, preprocessing, model selection, model construction, and evaluation phases.\nThe recommender system employs three standalone models, implementing collaboration filtering methods.","title":"Novel Recommender"},{"content":"Github link ðŸ’¾Backend\nCredit Transformer model is introduced in this paper\nSkill invlove Python Pytorch Deep Learning\nModel 1:\nHand Coded Transformer with Multihead Self-Attention This is a hand-coded implementation of Transformer with Multihead Self-Attention.\nThe main goal is to help me better understand the underline logic of transformer model\nOverview The Transformer model revolutionized sequence modeling by employing the self-attention mechanism. This mechanism enables the model to capture long-range dependencies efficiently, making it highly effective for processing sequential data. The core idea of the Transformer lies in its ability to process input sequences in parallel, resulting in faster computation and better capture of global information.\nThe key component of the Transformer model is the multihead self-attention mechanism. This mechanism allows the model to attend to different parts of the input sequence simultaneously, utilizing multiple attention heads. By leveraging multiple attention heads, the model can learn diverse representations and capture various dependencies within the sequence.\nFeatures Hand-coded implementation of the Transformer model with multihead self-attention Encoder-decoder architecture for sequence-to-sequence tasks Self-attention mechanism for capturing global dependencies Multihead attention for learning diverse representations Positional encoding to incorporate word order information Feed-forward networks for non-linear transformations Customizable hyperparameters for fine-tuning the model Training procedures for optimizing the model\u0026rsquo;s performance ","permalink":"https://hanl1223.github.io/resume/projects/hand_coded/","summary":"Github link ðŸ’¾Backend\nCredit Transformer model is introduced in this paper\nSkill invlove Python Pytorch Deep Learning\nModel 1:\nHand Coded Transformer with Multihead Self-Attention This is a hand-coded implementation of Transformer with Multihead Self-Attention.\nThe main goal is to help me better understand the underline logic of transformer model\nOverview The Transformer model revolutionized sequence modeling by employing the self-attention mechanism. This mechanism enables the model to capture long-range dependencies efficiently, making it highly effective for processing sequential data.","title":"Hand Coded"},{"content":"link ðŸ”— Dashborad\nCredit The dataset is downloaded from DATABASEURL\nThe reduced version used in this project is upladed as API form DATAURL\nSkill invlove Python MAGE Data engineering Googld Cloud Platform\nDescription This is a end to end project using a sample data taken from the NYC TLC Trip report invovle using python for initial clearning, using MAGE for construct data pipeline and data visulization using looker studio.\nSTEPS To start, I have downloaded the data and uploaded it as a data in google cloud bucket, it can also be made public so we can download it as an API\nUsing build in template from mage we can apply the following template to extract transform and load the data in to GCP\u0026rsquo;s BIGQuery\nThen we can perfrom SQL query to shape tables for visulization as needed\nKey Takeaways Hands on experience in data engineering.\nUtilized Python for data cleaning and transformation tasks, ensuring data quality and integrity.\nEnhanced understanding of SQL querying and database management, specifically working with GCP\u0026rsquo;s BigQuery.\nDeveloped in presenting data and insights through interactive dashboards using Looker Studio.(Further improvement require)\nDATA EXTACTION\nimport io import pandas as pd import requests if 'data_loader' not in globals(): from mage_ai.data_preparation.decorators import data_loader if 'test' not in globals(): from mage_ai.data_preparation.decorators import test @data_loader def load_data_from_api(*args, **kwargs): \u0026quot;\u0026quot;\u0026quot; Template for loading data from API \u0026quot;\u0026quot;\u0026quot; url = 'https://storage.googleapis.com/taxi-bucket1/uber_data.csv' response = requests.get(url) return pd.read_csv(io.StringIO(response.text), sep=',') @test def test_output(output, *args) -\u0026gt; None: \u0026quot;\u0026quot;\u0026quot; Template code for testing the output of the block. \u0026quot;\u0026quot;\u0026quot; assert output is not None, 'The output is undefined' DATA TRANSFORMATION\nHere we use the data loaded via api and perfrom data transformation\nimport pandas as pd if 'transformer' not in globals(): from mage_ai.data_preparation.decorators import transformer if 'test' not in globals(): from mage_ai.data_preparation.decorators import test @transformer def transform(df, *args, **kwargs): \u0026quot;\u0026quot;\u0026quot; Template code for a transformer block. Add more parameters to this function if this block has multiple parent blocks. There should be one parameter for each output variable from each parent block. Args: data: The output from the upstream parent block args: The output from any additional upstream blocks (if applicable) Returns: Anything (e.g. data frame, dictionary, array, int, str, etc.) \u0026quot;\u0026quot;\u0026quot; # Specify your transformation logic here df['tpep_pickup_datetime'] = pd.to_datetime(df['tpep_pickup_datetime']) df['tpep_dropoff_datetime'] = pd.to_datetime(df['tpep_dropoff_datetime']) datetime_dim = df[['tpep_pickup_datetime','tpep_dropoff_datetime']].drop_duplicates().reset_index(drop=True) datetime_dim['pick_hour'] = datetime_dim['tpep_pickup_datetime'].dt.hour datetime_dim['pick_day'] = datetime_dim['tpep_pickup_datetime'].dt.day datetime_dim['pick_month'] = datetime_dim['tpep_pickup_datetime'].dt.month datetime_dim['pick_year'] = datetime_dim['tpep_pickup_datetime'].dt.year datetime_dim['pick_weekday'] = datetime_dim['tpep_pickup_datetime'].dt.weekday datetime_dim['drop_hour'] = datetime_dim['tpep_dropoff_datetime'].dt.hour datetime_dim['drop_day'] = datetime_dim['tpep_dropoff_datetime'].dt.day datetime_dim['drop_month'] = datetime_dim['tpep_dropoff_datetime'].dt.month datetime_dim['drop_year'] = datetime_dim['tpep_dropoff_datetime'].dt.year datetime_dim['drop_weekday'] = datetime_dim['tpep_dropoff_datetime'].dt.weekday datetime_dim['datetime_id'] = datetime_dim.index datetime_dim = datetime_dim[['datetime_id', 'tpep_pickup_datetime', 'pick_hour', 'pick_day', 'pick_month', 'pick_year', 'pick_weekday', 'tpep_dropoff_datetime', 'drop_hour', 'drop_day', 'drop_month', 'drop_year', 'drop_weekday']] passenger_count_dim = df[['passenger_count']].drop_duplicates().reset_index(drop=True) passenger_count_dim['passenger_count_id'] = passenger_count_dim.index passenger_count_dim = passenger_count_dim[['passenger_count_id','passenger_count']] trip_distance_dim = df[['trip_distance']].drop_duplicates().reset_index(drop=True) trip_distance_dim['trip_distance_id'] = trip_distance_dim.index trip_distance_dim = trip_distance_dim[['trip_distance_id','trip_distance']] rate_code_type = { 1:\u0026quot;Standard rate\u0026quot;, 2:\u0026quot;JFK\u0026quot;, 3:\u0026quot;Newark\u0026quot;, 4:\u0026quot;Nassau or Westchester\u0026quot;, 5:\u0026quot;Negotiated fare\u0026quot;, 6:\u0026quot;Group ride\u0026quot; } rate_code_dim = df[['RatecodeID']].drop_duplicates().reset_index(drop=True) rate_code_dim['rate_code_id'] = rate_code_dim.index rate_code_dim['rate_code_name'] = rate_code_dim['RatecodeID'].map(rate_code_type) rate_code_dim = rate_code_dim[['rate_code_id','RatecodeID','rate_code_name']] pickup_location_dim = df[['pickup_longitude', 'pickup_latitude']].drop_duplicates().reset_index(drop=True) pickup_location_dim['pickup_location_id'] = pickup_location_dim.index pickup_location_dim = pickup_location_dim[['pickup_location_id','pickup_latitude','pickup_longitude']] dropoff_location_dim = df[['dropoff_longitude', 'dropoff_latitude']].drop_duplicates().reset_index(drop=True) dropoff_location_dim['dropoff_location_id'] = dropoff_location_dim.index dropoff_location_dim = dropoff_location_dim[['dropoff_location_id','dropoff_latitude','dropoff_longitude']] payment_type_name = { 1:\u0026quot;Credit card\u0026quot;, 2:\u0026quot;Cash\u0026quot;, 3:\u0026quot;No charge\u0026quot;, 4:\u0026quot;Dispute\u0026quot;, 5:\u0026quot;Unknown\u0026quot;, 6:\u0026quot;Voided trip\u0026quot; } payment_type_dim = df[['payment_type']].drop_duplicates().reset_index(drop=True) payment_type_dim['payment_type_id'] = payment_type_dim.index payment_type_dim['payment_type_name'] = payment_type_dim['payment_type'].map(payment_type_name) payment_type_dim = payment_type_dim[['payment_type_id','payment_type','payment_type_name']] fact_table = df.merge(passenger_count_dim, on='passenger_count') \\ .merge(trip_distance_dim, on='trip_distance') \\ .merge(rate_code_dim, on='RatecodeID') \\ .merge(pickup_location_dim, on=['pickup_longitude', 'pickup_latitude']) \\ .merge(dropoff_location_dim, on=['dropoff_longitude', 'dropoff_latitude'])\\ .merge(datetime_dim, on=['tpep_pickup_datetime','tpep_dropoff_datetime']) \\ .merge(payment_type_dim, on='payment_type') \\ [['VendorID', 'datetime_id', 'passenger_count_id', 'trip_distance_id', 'rate_code_id', 'store_and_fwd_flag', 'pickup_location_id', 'dropoff_location_id', 'payment_type_id', 'fare_amount', 'extra', 'mta_tax', 'tip_amount', 'tolls_amount', 'improvement_surcharge', 'total_amount']] # Return dictionary like value so we can create 1 table for each combination return {\u0026quot;datetime_dim\u0026quot;:datetime_dim.to_dict(orient=\u0026quot;dict\u0026quot;), \u0026quot;passenger_count_dim\u0026quot;:passenger_count_dim.to_dict(orient=\u0026quot;dict\u0026quot;), \u0026quot;trip_distance_dim\u0026quot;:trip_distance_dim.to_dict(orient=\u0026quot;dict\u0026quot;), \u0026quot;rate_code_dim\u0026quot;:rate_code_dim.to_dict(orient=\u0026quot;dict\u0026quot;), \u0026quot;pickup_location_dim\u0026quot;:pickup_location_dim.to_dict(orient=\u0026quot;dict\u0026quot;), \u0026quot;dropoff_location_dim\u0026quot;:dropoff_location_dim.to_dict(orient=\u0026quot;dict\u0026quot;), \u0026quot;payment_type_dim\u0026quot;:payment_type_dim.to_dict(orient=\u0026quot;dict\u0026quot;), \u0026quot;fact_table\u0026quot;:fact_table.to_dict(orient=\u0026quot;dict\u0026quot;)} @test def test_output(output, *args) -\u0026gt; None: \u0026quot;\u0026quot;\u0026quot; Template code for testing the output of the block. \u0026quot;\u0026quot;\u0026quot; assert output is not None, 'The output is undefined' DATALOADER\nfrom mage_ai.data_preparation.repo_manager import get_repo_path from mage_ai.io.bigquery import BigQuery from mage_ai.io.config import ConfigFileLoader from pandas import DataFrame from os import path if 'data_exporter' not in globals(): from mage_ai.data_preparation.decorators import data_exporter @data_exporter def export_data_to_big_query(data, **kwargs) -\u0026gt; None: \u0026quot;\u0026quot;\u0026quot; Template for exporting data to a BigQuery warehouse. Specify your configuration settings in 'io_config.yaml'. Docs: https://docs.mage.ai/design/data-loading#bigquery \u0026quot;\u0026quot;\u0026quot; config_path = path.join(get_repo_path(), 'io_config.yaml') config_profile = 'default' # K,V pair for each K, generate a table of V for key, value in data.items(): table_id = 'portfolio-pipeline.DEpipeline.{}'.format(key) BigQuery.with_config(ConfigFileLoader(config_path, config_profile)).export( DataFrame(value), print(key,value) table_id, if_exists='replace', # Specify resolution policy if table name already exists ) ","permalink":"https://hanl1223.github.io/resume/projects/de/","summary":"link ðŸ”— Dashborad\nCredit The dataset is downloaded from DATABASEURL\nThe reduced version used in this project is upladed as API form DATAURL\nSkill invlove Python MAGE Data engineering Googld Cloud Platform\nDescription This is a end to end project using a sample data taken from the NYC TLC Trip report invovle using python for initial clearning, using MAGE for construct data pipeline and data visulization using looker studio.\nSTEPS To start, I have downloaded the data and uploaded it as a data in google cloud bucket, it can also be made public so we can download it as an API","title":"NYC for hire car Dash Borad"},{"content":"Description Collaborated with all departments to ensure a seamless arrival experience for guests and groups. Successfully resolved over 90% of customer complaints within 24 hours and maintained an overall satisfaction score of 90/100. Worked with director of sales and revenue to provide demand insight and assist in creating monthly forecasts. Adjusted inventory to reflect daily demand and to optimise revenue. ","permalink":"https://hanl1223.github.io/resume/experience/como/","summary":"Description Collaborated with all departments to ensure a seamless arrival experience for guests and groups. Successfully resolved over 90% of customer complaints within 24 hours and maintained an overall satisfaction score of 90/100. Worked with director of sales and revenue to provide demand insight and assist in creating monthly forecasts. Adjusted inventory to reflect daily demand and to optimise revenue. ","title":"Guest Service Agent/ Duty Manager/ Reservation Supervisor(secondment)"},{"content":"Description Processing reservations and related enquiry from multiple channels Liaise with all related department to ensure every guest and group has a smooth arrival experience. Flagged VIP guest information, preferences, and communicated to related departments. Conducted market research and competitor analysis for oversea travel trend. ","permalink":"https://hanl1223.github.io/resume/experience/pullman/","summary":"Description Processing reservations and related enquiry from multiple channels Liaise with all related department to ensure every guest and group has a smooth arrival experience. Flagged VIP guest information, preferences, and communicated to related departments. Conducted market research and competitor analysis for oversea travel trend. ","title":"Inbound Reservation Sales Agent"},{"content":"Description Processed reservations and related inquiries from multiple channels. Assisted the reservation manager in inventory control by balancing inventory and managing overbooking, prepared requested reports including daily pick-up and competitor benchmarking. Maintained close communication with all departments to ensure a smooth arrival experience for guests. Collected requested information and assisted the revenue department to conduct price and marketing analysis. Assisted with integration and system switch for Starwood to Marriott across three large-scale properties. Assisted with general reservations agent\u0026rsquo;s role, including daily arrival check, investigation of open guest accounts and other ad-hoc tasks assigned by the management team. Assisted the revenue manager to monitor demand and provide insight for pricing strategy. ","permalink":"https://hanl1223.github.io/resume/experience/marriott/","summary":"Description Processed reservations and related inquiries from multiple channels. Assisted the reservation manager in inventory control by balancing inventory and managing overbooking, prepared requested reports including daily pick-up and competitor benchmarking. Maintained close communication with all departments to ensure a smooth arrival experience for guests. Collected requested information and assisted the revenue department to conduct price and marketing analysis. Assisted with integration and system switch for Starwood to Marriott across three large-scale properties.","title":"Revenue Intern \u0026 Reservations Agent"},{"content":"More to add here but I will add some of my favorite dog picture here. Many will be Golden Retriever\n","permalink":"https://hanl1223.github.io/resume/search/","summary":"search","title":"About ME"}]