[{"content":"Github link ðŸ”— Function URL ðŸ–¥ Frontend ðŸ’¾Backend\nCredit Members of team TP8 for subject FIT5120/22\nSkill invlove Python Pytorch Deep Learning Docker AWS Azure\nDescription As the final project of my Master Degree, this website is to tackle the topic of digital poverty by applying various functions. I mainly take part in the avoidng scam function where i used a fine-tuned BERT model to predict the likelihood of a text message received by a senior is a scam or not. To add more content to the result, I also displayed the words that has the highest attention score to help our target audiences understand how the result is driven.\nKey Takeaways Combining DS skill to a real project. Experience on merging model develpment with backend(Django) and frontend(React). Collaborative Development. Experience a full-stack project development with a team,and using Github for version control in a team setting Agile Management. Here I also exposed to the full agile software development cycle of a product Cloud Computing. In-depth knowledge and hands on experience on both AWS(EC2,Route52,S3) and Azure (Machine Learning Studio,Virtual machines) New Domain knowledge. First time expose to developing a product in a more public domain setting ","permalink":"https://hanl1223.github.io/resume/projects/seniordigi/","summary":"Github link ðŸ”— Function URL ðŸ–¥ Frontend ðŸ’¾Backend\nCredit Members of team TP8 for subject FIT5120/22\nSkill invlove Python Pytorch Deep Learning Docker AWS Azure\nDescription As the final project of my Master Degree, this website is to tackle the topic of digital poverty by applying various functions. I mainly take part in the avoidng scam function where i used a fine-tuned BERT model to predict the likelihood of a text message received by a senior is a scam or not.","title":"Bridging the Digital Divide"},{"content":"Github link ðŸ”— Function URL ðŸ–¥ Code\nCredit Members of team TP8 for subject FIT5120/22\nSkill invlove Python Pandas Machine Learning\nDescription This is an additional function be added to the final project, a password strength evaluator, based on the dataset A and dataset B\nKey Takeaways Combining DS skill to a real project. Experience on merging model develpment with backend(Django) and frontend(React). Agile Management. Here I also exposed to the full agile software development cycle of a product Function Code ","permalink":"https://hanl1223.github.io/resume/projects/security_check/","summary":"Github link ðŸ”— Function URL ðŸ–¥ Code\nCredit Members of team TP8 for subject FIT5120/22\nSkill invlove Python Pandas Machine Learning\nDescription This is an additional function be added to the final project, a password strength evaluator, based on the dataset A and dataset B\nKey Takeaways Combining DS skill to a real project. Experience on merging model develpment with backend(Django) and frontend(React). Agile Management. Here I also exposed to the full agile software development cycle of a product Function Code ","title":"Password Strength Validation"},{"content":"link ðŸ”— Dashborad\nCredit The dataset is downloaded fromDATABASEURL\nThe reduced version used in this project is upladed as API form DATAURL\nSkill invlove Python MAGE Data engineering Googld Cloud Platform\nDescription This is a end to end project using a sample data taken from the NYC TLC Trip report invovle using python for initial clearning, using MAGE for construct data pipeline and data visulization using looker studio.\nSTEPS To start, I have downloaded the data and uploaded it as a data in google cloud bucket, it can also be made public so we can download it as an API\nUsing build in template from mage we can apply the following template to extract transform and load the data in to GCP\u0026rsquo;s BIGQuery\nThen we can perfrom SQL query to shape tables for visulization as needed\nKey Takeaways Hands on experience in data engineering.\nUtilized Python for data cleaning and transformation tasks, ensuring data quality and integrity.\nEnhanced understanding of SQL querying and database management, specifically working with GCP\u0026rsquo;s BigQuery.\nDeveloped in presenting data and insights through interactive dashboards using Looker Studio.(Further improvement require)\nDATA EXTACTION\nimport io import pandas as pd import requests if 'data_loader' not in globals(): from mage_ai.data_preparation.decorators import data_loader if 'test' not in globals(): from mage_ai.data_preparation.decorators import test @data_loader def load_data_from_api(*args, **kwargs): \u0026quot;\u0026quot;\u0026quot; Template for loading data from API \u0026quot;\u0026quot;\u0026quot; url = 'https://storage.googleapis.com/taxi-bucket1/uber_data.csv' response = requests.get(url) return pd.read_csv(io.StringIO(response.text), sep=',') @test def test_output(output, *args) -\u0026gt; None: \u0026quot;\u0026quot;\u0026quot; Template code for testing the output of the block. \u0026quot;\u0026quot;\u0026quot; assert output is not None, 'The output is undefined' DATA TRANSFORMATION\nHere we use the data loaded via api and perfrom data transformation\nimport pandas as pd if 'transformer' not in globals(): from mage_ai.data_preparation.decorators import transformer if 'test' not in globals(): from mage_ai.data_preparation.decorators import test @transformer def transform(df, *args, **kwargs): \u0026quot;\u0026quot;\u0026quot; Template code for a transformer block. Add more parameters to this function if this block has multiple parent blocks. There should be one parameter for each output variable from each parent block. Args: data: The output from the upstream parent block args: The output from any additional upstream blocks (if applicable) Returns: Anything (e.g. data frame, dictionary, array, int, str, etc.) \u0026quot;\u0026quot;\u0026quot; # Specify your transformation logic here df['tpep_pickup_datetime'] = pd.to_datetime(df['tpep_pickup_datetime']) df['tpep_dropoff_datetime'] = pd.to_datetime(df['tpep_dropoff_datetime']) datetime_dim = df[['tpep_pickup_datetime','tpep_dropoff_datetime']].drop_duplicates().reset_index(drop=True) datetime_dim['pick_hour'] = datetime_dim['tpep_pickup_datetime'].dt.hour datetime_dim['pick_day'] = datetime_dim['tpep_pickup_datetime'].dt.day datetime_dim['pick_month'] = datetime_dim['tpep_pickup_datetime'].dt.month datetime_dim['pick_year'] = datetime_dim['tpep_pickup_datetime'].dt.year datetime_dim['pick_weekday'] = datetime_dim['tpep_pickup_datetime'].dt.weekday datetime_dim['drop_hour'] = datetime_dim['tpep_dropoff_datetime'].dt.hour datetime_dim['drop_day'] = datetime_dim['tpep_dropoff_datetime'].dt.day datetime_dim['drop_month'] = datetime_dim['tpep_dropoff_datetime'].dt.month datetime_dim['drop_year'] = datetime_dim['tpep_dropoff_datetime'].dt.year datetime_dim['drop_weekday'] = datetime_dim['tpep_dropoff_datetime'].dt.weekday datetime_dim['datetime_id'] = datetime_dim.index datetime_dim = datetime_dim[['datetime_id', 'tpep_pickup_datetime', 'pick_hour', 'pick_day', 'pick_month', 'pick_year', 'pick_weekday', 'tpep_dropoff_datetime', 'drop_hour', 'drop_day', 'drop_month', 'drop_year', 'drop_weekday']] passenger_count_dim = df[['passenger_count']].drop_duplicates().reset_index(drop=True) passenger_count_dim['passenger_count_id'] = passenger_count_dim.index passenger_count_dim = passenger_count_dim[['passenger_count_id','passenger_count']] trip_distance_dim = df[['trip_distance']].drop_duplicates().reset_index(drop=True) trip_distance_dim['trip_distance_id'] = trip_distance_dim.index trip_distance_dim = trip_distance_dim[['trip_distance_id','trip_distance']] rate_code_type = { 1:\u0026quot;Standard rate\u0026quot;, 2:\u0026quot;JFK\u0026quot;, 3:\u0026quot;Newark\u0026quot;, 4:\u0026quot;Nassau or Westchester\u0026quot;, 5:\u0026quot;Negotiated fare\u0026quot;, 6:\u0026quot;Group ride\u0026quot; } rate_code_dim = df[['RatecodeID']].drop_duplicates().reset_index(drop=True) rate_code_dim['rate_code_id'] = rate_code_dim.index rate_code_dim['rate_code_name'] = rate_code_dim['RatecodeID'].map(rate_code_type) rate_code_dim = rate_code_dim[['rate_code_id','RatecodeID','rate_code_name']] pickup_location_dim = df[['pickup_longitude', 'pickup_latitude']].drop_duplicates().reset_index(drop=True) pickup_location_dim['pickup_location_id'] = pickup_location_dim.index pickup_location_dim = pickup_location_dim[['pickup_location_id','pickup_latitude','pickup_longitude']] dropoff_location_dim = df[['dropoff_longitude', 'dropoff_latitude']].drop_duplicates().reset_index(drop=True) dropoff_location_dim['dropoff_location_id'] = dropoff_location_dim.index dropoff_location_dim = dropoff_location_dim[['dropoff_location_id','dropoff_latitude','dropoff_longitude']] payment_type_name = { 1:\u0026quot;Credit card\u0026quot;, 2:\u0026quot;Cash\u0026quot;, 3:\u0026quot;No charge\u0026quot;, 4:\u0026quot;Dispute\u0026quot;, 5:\u0026quot;Unknown\u0026quot;, 6:\u0026quot;Voided trip\u0026quot; } payment_type_dim = df[['payment_type']].drop_duplicates().reset_index(drop=True) payment_type_dim['payment_type_id'] = payment_type_dim.index payment_type_dim['payment_type_name'] = payment_type_dim['payment_type'].map(payment_type_name) payment_type_dim = payment_type_dim[['payment_type_id','payment_type','payment_type_name']] fact_table = df.merge(passenger_count_dim, on='passenger_count') \\ .merge(trip_distance_dim, on='trip_distance') \\ .merge(rate_code_dim, on='RatecodeID') \\ .merge(pickup_location_dim, on=['pickup_longitude', 'pickup_latitude']) \\ .merge(dropoff_location_dim, on=['dropoff_longitude', 'dropoff_latitude'])\\ .merge(datetime_dim, on=['tpep_pickup_datetime','tpep_dropoff_datetime']) \\ .merge(payment_type_dim, on='payment_type') \\ [['VendorID', 'datetime_id', 'passenger_count_id', 'trip_distance_id', 'rate_code_id', 'store_and_fwd_flag', 'pickup_location_id', 'dropoff_location_id', 'payment_type_id', 'fare_amount', 'extra', 'mta_tax', 'tip_amount', 'tolls_amount', 'improvement_surcharge', 'total_amount']] # Return dictionary like value so we can create 1 table for each combination return {\u0026quot;datetime_dim\u0026quot;:datetime_dim.to_dict(orient=\u0026quot;dict\u0026quot;), \u0026quot;passenger_count_dim\u0026quot;:passenger_count_dim.to_dict(orient=\u0026quot;dict\u0026quot;), \u0026quot;trip_distance_dim\u0026quot;:trip_distance_dim.to_dict(orient=\u0026quot;dict\u0026quot;), \u0026quot;rate_code_dim\u0026quot;:rate_code_dim.to_dict(orient=\u0026quot;dict\u0026quot;), \u0026quot;pickup_location_dim\u0026quot;:pickup_location_dim.to_dict(orient=\u0026quot;dict\u0026quot;), \u0026quot;dropoff_location_dim\u0026quot;:dropoff_location_dim.to_dict(orient=\u0026quot;dict\u0026quot;), \u0026quot;payment_type_dim\u0026quot;:payment_type_dim.to_dict(orient=\u0026quot;dict\u0026quot;), \u0026quot;fact_table\u0026quot;:fact_table.to_dict(orient=\u0026quot;dict\u0026quot;)} @test def test_output(output, *args) -\u0026gt; None: \u0026quot;\u0026quot;\u0026quot; Template code for testing the output of the block. \u0026quot;\u0026quot;\u0026quot; assert output is not None, 'The output is undefined' DATALOADER\nfrom mage_ai.data_preparation.repo_manager import get_repo_path from mage_ai.io.bigquery import BigQuery from mage_ai.io.config import ConfigFileLoader from pandas import DataFrame from os import path if 'data_exporter' not in globals(): from mage_ai.data_preparation.decorators import data_exporter @data_exporter def export_data_to_big_query(data, **kwargs) -\u0026gt; None: \u0026quot;\u0026quot;\u0026quot; Template for exporting data to a BigQuery warehouse. Specify your configuration settings in 'io_config.yaml'. Docs: https://docs.mage.ai/design/data-loading#bigquery \u0026quot;\u0026quot;\u0026quot; config_path = path.join(get_repo_path(), 'io_config.yaml') config_profile = 'default' # K,V pair for each K, generate a table of V for key, value in data.items(): table_id = 'portfolio-pipeline.DEpipeline.{}'.format(key) BigQuery.with_config(ConfigFileLoader(config_path, config_profile)).export( DataFrame(value), print(key,value) table_id, if_exists='replace', # Specify resolution policy if table name already exists ) ","permalink":"https://hanl1223.github.io/resume/projects/de/","summary":"link ðŸ”— Dashborad\nCredit The dataset is downloaded fromDATABASEURL\nThe reduced version used in this project is upladed as API form DATAURL\nSkill invlove Python MAGE Data engineering Googld Cloud Platform\nDescription This is a end to end project using a sample data taken from the NYC TLC Trip report invovle using python for initial clearning, using MAGE for construct data pipeline and data visulization using looker studio.\nSTEPS To start, I have downloaded the data and uploaded it as a data in google cloud bucket, it can also be made public so we can download it as an API","title":"NYC for hire car Dash Borad"},{"content":"Description Collaborated with all departments to ensure a seamless arrival experience for guests and groups. Successfully resolved over 90% of customer complaints within 24 hours and maintained an overall satisfaction score of 90/100. Worked with director of sales and revenue to provide demand insight and assist in creating monthly forecasts. Adjusted inventory to reflect daily demand and to optimise revenue. ","permalink":"https://hanl1223.github.io/resume/experience/como/","summary":"Description Collaborated with all departments to ensure a seamless arrival experience for guests and groups. Successfully resolved over 90% of customer complaints within 24 hours and maintained an overall satisfaction score of 90/100. Worked with director of sales and revenue to provide demand insight and assist in creating monthly forecasts. Adjusted inventory to reflect daily demand and to optimise revenue. ","title":"Guest Service Agent/ Duty Manager/ Reservation Supervisor(secondment)"},{"content":"Description Processing reservations and related enquiry from multiple channels Liaise with all related department to ensure every guest and group has a smooth arrival experience. Flagged VIP guest information, preferences, and communicated to related departments. Conducted market research and competitor analysis for oversea travel trend. ","permalink":"https://hanl1223.github.io/resume/experience/pullman/","summary":"Description Processing reservations and related enquiry from multiple channels Liaise with all related department to ensure every guest and group has a smooth arrival experience. Flagged VIP guest information, preferences, and communicated to related departments. Conducted market research and competitor analysis for oversea travel trend. ","title":"Inbound Reservation Sales Agent"},{"content":"Description Processed reservations and related inquiries from multiple channels. Assisted the reservation manager in inventory control by balancing inventory and managing overbooking, prepared requested reports including daily pick-up and competitor benchmarking. Maintained close communication with all departments to ensure a smooth arrival experience for guests. Collected requested information and assisted the revenue department to conduct price and marketing analysis. Assisted with integration and system switch for Starwood to Marriott across three large-scale properties. Assisted with general reservations agent\u0026rsquo;s role, including daily arrival check, investigation of open guest accounts and other ad-hoc tasks assigned by the management team. Assisted the revenue manager to monitor demand and provide insight for pricing strategy. ","permalink":"https://hanl1223.github.io/resume/experience/marriott/","summary":"Description Processed reservations and related inquiries from multiple channels. Assisted the reservation manager in inventory control by balancing inventory and managing overbooking, prepared requested reports including daily pick-up and competitor benchmarking. Maintained close communication with all departments to ensure a smooth arrival experience for guests. Collected requested information and assisted the revenue department to conduct price and marketing analysis. Assisted with integration and system switch for Starwood to Marriott across three large-scale properties.","title":"Revenue Intern \u0026 Reservations Agent"}]